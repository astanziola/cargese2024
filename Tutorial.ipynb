{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b10af3d-d4c8-4c56-bb3d-656a50cf2e04",
   "metadata": {},
   "source": [
    "# Differentiable acoustic simulations using `jwave`\n",
    "\n",
    "Welcome to this advanced tutorial on acoustic wave simulation and optimization using jwave. This session will explore the integration of JAX's powerful automatic differentiation capabilities with acoustic modeling, opening up new avenues for research in wave physics and acoustic applications.\n",
    "Our objectives for this tutorial are to:\n",
    "\n",
    "1. Introduce the fundamentals of setting up and running acoustic simulations in jwave\n",
    "2. Demonstrate the application of JAX's automatic differentiation to acoustic problems\n",
    "3. Explore examples of applications in acoustic field optimization, sensitivity analysis and uncertainty quantification\n",
    "\n",
    "By the end of this session, you will have gained hands-on experience with state-of-the-art tools for computational acoustics, preparing you for cutting-edge research in this field. \n",
    "\n",
    "Let's begin üöÄ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07112ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9334e5b-eadd-4d2c-9980-cc18af083da7",
   "metadata": {},
   "source": [
    "#### Preliminars\n",
    "\n",
    "If you have not done it already, run the cell below to install all the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c009c53-3265-48af-856b-f23f4efce9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r https://github.com/astanziola/cargese2024/blob/main/requirements.txt?raw=true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5399ba9-83b8-4572-ba29-7cfe98a13a06",
   "metadata": {},
   "source": [
    "Also, if you are on a machine with an NVIDIA GPU, install jax with GPU support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c11ba84-43ef-4851-90f0-da6051aff0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jax[cuda12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bd9aaa-ade8-402c-821a-d6f43e15270b",
   "metadata": {},
   "source": [
    "Lastly, if you are on Colab, install the tutorial package by running the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e005e63f-fe61-4ff2-b69b-2e0deb626186",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/astanziola/cargese2024.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b240b61",
   "metadata": {},
   "source": [
    "## A primer on `jax`\n",
    "\n",
    "### Introduction to JAX\n",
    "JAX is a powerful library for high-performance numerical computing and machine learning research. It combines NumPy's familiar API with the benefits of automatic differentiation and accelerated computation on GPUs and TPUs.\n",
    "\n",
    "\n",
    "### Key features of JAX:\n",
    "\n",
    "1. **NumPy-like API**: Familiar syntax for NumPy users\n",
    "2. **Automatic Differentiation**: Compute derivatives of any Python function\n",
    "3. **Just-In-Time (JIT) compilation**: Optimize and compile your code for faster execution\n",
    "4. **Vectorization**: Easily parallelize operations across multiple devices\n",
    "5. **GPU/TPU support**: Seamlessly run computations on accelerated hardware\n",
    "\n",
    "Let's start by importing JAX and checking its version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da12cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Available devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f38eb5",
   "metadata": {},
   "source": [
    "### Basic JAX Operations\n",
    "\n",
    "JAX provides a NumPy-like interface through the `jax.numpy` module, which is typically imported as `jnp`. This allows users familiar with NumPy to transition easily to JAX while gaining access to its powerful features like automatic differentiation and JIT compilation.\n",
    "\n",
    "In JAX, we create arrays using `jnp.array()` or other array creation functions. Let's look at some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96da42c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 1D array\n",
    "a = jnp.array([1, 2, 3, 4, 5])\n",
    "print(\"1D array:\", a)\n",
    "\n",
    "# Create a 2D array\n",
    "b = jnp.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(\"2D array:\\n\", b)\n",
    "\n",
    "# Create an array of zeros\n",
    "c = jnp.zeros((3, 3))\n",
    "print(\"Array of zeros:\\n\", c)\n",
    "\n",
    "# Create an array of ones\n",
    "d = jnp.ones((2, 2))\n",
    "print(\"Array of ones:\\n\", d)\n",
    "\n",
    "# Create an array with a range of values\n",
    "e = jnp.arange(10)\n",
    "print(\"Array with range:\", e)\n",
    "\n",
    "# Create a linearly spaced array\n",
    "f = jnp.linspace(0, 1, 5)\n",
    "print(\"Linearly spaced array:\", f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec7f414",
   "metadata": {},
   "source": [
    "> *Note that unlike NumPy arrays, JAX arrays are immutable (more on this later). This means that once created, their values cannot be changed in-place.*\n",
    "\n",
    "JAX supports a wide range of array operations, similar to NumPy. Here are some common operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bb2db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = jnp.array([1, 2, 3, 4, 5])\n",
    "b = jnp.array([5, 4, 3, 2, 1]).astype(jnp.float32) # Can specify the data type!\n",
    "\n",
    "# Addition\n",
    "print(\"a + b =\", a + b)\n",
    "\n",
    "# Subtraction\n",
    "print(\"a - b =\", a - b)\n",
    "\n",
    "# Multiplication\n",
    "print(\"a * b =\", a * b)\n",
    "\n",
    "# Division\n",
    "print(\"a / b =\", a / b)\n",
    "\n",
    "# Exponentiation\n",
    "print(\"a ** 2 =\", a ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66885f0c",
   "metadata": {},
   "source": [
    "It also provides a variety of mathematical functions that operate on arrays, either elementwise or in aggregate. Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c55bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sin(a) =\", jnp.sin(a))\n",
    "print(\"sqrt(a) =\", jnp.sqrt(a))\n",
    "\n",
    "print(\"Mean of a:\", jnp.mean(a))\n",
    "print(\"Min of a:\", jnp.min(a))\n",
    "print(\"Standard deviation of a:\", jnp.std(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cc97a1-efe5-467e-bc03-0f8cf51dab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to import some required packages\n",
    "from jwavetutorial import marking\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955cc8b9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "__üìù Exercise__\n",
    "\n",
    "Try to write the code to create an array `x` containing the first 10 even numbers `[2, 4, 6, ..., 20]` and an array `y` containing the first 10 odd numbers `[1, 3, 5, ..., 19]`. Hint: Use <a href=\"https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.arange.html\"><code>jax.numpy.arange</code></a> or <a href=\"https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.linspace.html\"><code>jax.numpy.linspace</code></a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f985d1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_arrays():\n",
    "  # --- Add your code here ---\n",
    "  x = ...\n",
    "  y = ...\n",
    "  # ---------------------------\n",
    "  return x, y\n",
    "\n",
    "# Verify the function\n",
    "marking.verify_create_arrays(create_arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5afc35",
   "metadata": {},
   "source": [
    "Like `numpy`, JAX provides functions to manipulate arrays, such as reshaping, concatenation, and splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd7b6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape an array\n",
    "g = jnp.arange(12).reshape(3, 4)\n",
    "print(\"Reshaped array:\\n\", g)\n",
    "\n",
    "# Transpose an array\n",
    "print(\"Transposed array:\\n\", g.T)\n",
    "\n",
    "# Flatten an array\n",
    "print(\"Flattened array:\", g.flatten())\n",
    "\n",
    "# Concatenate arrays\n",
    "h = jnp.concatenate([a, b])\n",
    "print(\"Concatenated array:\", h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd76d60c",
   "metadata": {},
   "source": [
    "Broadcasting is a powerful feature in JAX (and NumPy) that allows operations between arrays of different shapes. JAX will automatically broadcast arrays to compatible shapes when possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302919a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting example\n",
    "i = jnp.array([1, 2, 3])\n",
    "j = jnp.array([[1], [2], [3]])\n",
    "\n",
    "print(\"i + j:\\n\", i + j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f6440d",
   "metadata": {},
   "source": [
    "In this example, `i` is broadcast to match the shape of `j`, allowing element-wise addition.\n",
    "\n",
    "Finally, JAX arrays can be indexed and sliced similarly to NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1846b1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = jnp.arange(10)\n",
    "\n",
    "# Indexing\n",
    "print(\"k[3] =\", k[3])\n",
    "\n",
    "# Slicing\n",
    "print(\"k[2:5] =\", k[2:5])\n",
    "print(\"k[::2] =\", k[::2])  # Every second element\n",
    "\n",
    "# Boolean indexing\n",
    "mask = k > 5\n",
    "print(\"k[k > 5] =\", k[mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949d15f5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "__üìù Exercise__\n",
    "\n",
    "Try to write the code to create a 2D array containing the following 3x3 matrix:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "and then calculate both its matrix square and elementwise square\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a361cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_exercise():\n",
    "  # --- Add your code here ---\n",
    "  matrix_square = ...\n",
    "  matrix_elementwise_squared = ...\n",
    "  # ---------------------------\n",
    "  return matrix_square, matrix_elementwise_squared\n",
    "\n",
    "# Verify the function\n",
    "marking.verify_matrix_exercise(matrix_exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91e4560",
   "metadata": {},
   "source": [
    "#### The Catch: Immutability in JAX\n",
    "\n",
    "One of the most important concepts to understand when working with JAX is that its arrays are immutable. This is a significant difference from NumPy arrays and can catch newcomers off guard. \n",
    "\n",
    "In JAX, once an array is created, its contents cannot be modified in-place. This means that operations that appear to modify an array actually create a new array with the modified values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3e0d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a JAX array\n",
    "x = jnp.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Attempt to modify the array in place\n",
    "# x[0] = 10  # This will raise an error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df901a35",
   "metadata": {},
   "source": [
    "If you run this code, you'll get a `TypeError` with a message like `\"JAX arrays are immutable; use x.at[idx].set(val) instead\"`0.\n",
    "\n",
    "Instead of in-place modifications, JAX provides functional update operations. Here's how you can achieve the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b13ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct way to update a JAX array\n",
    "y = x.at[0].set(10)\n",
    "print(\"Original x:\", x)\n",
    "print(\"New array y:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce06b99",
   "metadata": {},
   "source": [
    "In this example, x remains unchanged, and a new array y is created with the updated value.\n",
    "\n",
    "##### Implications for Your Code\n",
    "\n",
    "1. **No In-Place Operations**: Operations like `+=`, `-=`, etc., which modify arrays in-place in NumPy, create new arrays in JAX.\n",
    "\n",
    "2. **Functional Programming Style**: JAX encourages a more functional programming style, where you create new arrays instead of modifying existing ones.\n",
    "\n",
    "3. **Memory Usage**: Be aware that operations on large arrays will create new arrays, which can impact memory usage.\n",
    "\n",
    "4. **Performance Considerations**: Despite creating new arrays, JAX is optimized for these operations and can often perform them efficiently, especially when using JIT compilation (more on this soon)\n",
    "\n",
    "Here's an example of updating multiple elements in a JAX array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c051fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2D JAX array\n",
    "matrix = jnp.arange(9).reshape(3, 3)\n",
    "print(\"Original matrix:\\n\", matrix)\n",
    "\n",
    "# Update multiple elements\n",
    "new_matrix = matrix.at[0, :].set(jnp.array([10, 20, 30]))\n",
    "print(\"Updated matrix:\\n\", new_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0f82d9",
   "metadata": {},
   "source": [
    "In general, to get the most out of JAX, it's important to write code that aligns with JAX's functional programming paradigm. In particular, JAX works best with pure functions - functions that always produce the same output for a given input and don't have side effects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3c9ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good: Pure function\n",
    "def add_numbers(a, b):\n",
    "  return a + b\n",
    "\n",
    "# Bad: Impure function (avoid)\n",
    "total = 0\n",
    "def add_to_total(x):\n",
    "  global total\n",
    "  total += x\n",
    "  return total\n",
    "  \n",
    "print(\"Calling twice a pure function:\")\n",
    "print(add_numbers(1, 2))\n",
    "print(add_numbers(1, 2))\n",
    "\n",
    "print(\"Calling twice an impure function. Note that the result is different:\")\n",
    "print(add_to_total(1))\n",
    "print(add_to_total(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf59e026",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "__üìù Exercise__\n",
    "\n",
    "Create a 2D array `z` with shape `(5,2)` containing the numbers 1 to 10. Then, change the number corresponding to even-indexed row to -1. \n",
    "\n",
    "*Remember: differently from MATLAB, Python uses 0-based indexing.*\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af24ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_2d_exercise():\n",
    "  # --- Add your code here ---\n",
    "  matrix = ...\n",
    "  matrix_modified = ...\n",
    "  # ---------------------------\n",
    "  return matrix, matrix_modified\n",
    "\n",
    "# Verify the function0\n",
    "marking.verify_array_2d_exercise(array_2d_exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9cfeb4",
   "metadata": {},
   "source": [
    "After completing this section, you'll have a solid foundation in basic JAX operations and how to work with JAX's immutable arrays, which is crucial for writing correct and efficient JAX code. Immutability often leads to cleaner, more predictable code, especially in complex scientific computations where keeping track of state changes can be challenging. \n",
    "\n",
    "Like everything, the key to mastering JAX is practice. Don't hesitate to experiment with different array shapes, operations, and functions. The more you practice, the more comfortable you'll become with JAX's powerful features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36953405",
   "metadata": {},
   "source": [
    "### Just-In-Time (JIT) Compilation in JAX\n",
    "\n",
    "Just-In-Time (JIT) compilation is a powerful feature in JAX that can significantly speed up your code. JIT compilation works by translating your Python functions into optimized machine code at runtime, just before the function is executed. This can lead to substantial performance improvements, especially for numerical computations and when running on accelerated hardware like GPUs or TPUs.\n",
    "\n",
    "The primary benefits of JIT compilation in JAX include:\n",
    "\n",
    "1. Improved execution speed\n",
    "2. Reduced Python overhead\n",
    "3. Better utilization of hardware resources\n",
    "\n",
    "To use JIT compilation in JAX, you simply need to apply the `jax.jit` decorator to your function. Here's a basic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e0d01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Define a function\n",
    "def slow_f(x):\n",
    "    return x * 2\n",
    "\n",
    "# JIT-compile the function\n",
    "fast_f = jax.jit(slow_f)\n",
    "\n",
    "# Use the JIT-compiled function\n",
    "x = jnp.array([1, 2, 3, 4])\n",
    "result = fast_f(x)\n",
    "print(result)  # Output: [2 4 6 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6052f7c",
   "metadata": {},
   "source": [
    "You can also use `jit` as a decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0df6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def fast_g(x):\n",
    "    return x * 2\n",
    "\n",
    "result = fast_g(x)\n",
    "print(result)  # Output: [2 4 6 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce42e171",
   "metadata": {},
   "source": [
    "Here's a more complex example of using JIT compilation to speed up a Monte Carlo estimation of œÄ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59618890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "def monte_carlo_pi(n_points):\n",
    "    # Generate random points\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    points = jax.random.uniform(key, (n_points, 2), minval=-1, maxval=1)\n",
    "    \n",
    "    # Check if points are inside the unit circle\n",
    "    inside_circle = (points ** 2).sum(axis=1) <= 1\n",
    "    \n",
    "    # Estimate pi\n",
    "    pi_estimate = 4 * jnp.mean(inside_circle)\n",
    "    \n",
    "    return pi_estimate\n",
    "  \n",
    "# JIT-compiled version\n",
    "jit_monte_carlo_pi = jax.jit(monte_carlo_pi, static_argnums=(0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9f203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jwavetutorial import benchmark\n",
    "\n",
    "# Checking speedup\n",
    "n_points_list = [100_0000, 1_000_0000,  10_000_0000]\n",
    "benchmark.mc_pi_speed_comparison(monte_carlo_pi, jit_monte_carlo_pi, n_points_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca605f1",
   "metadata": {},
   "source": [
    "##### When to Use JIT\n",
    "\n",
    "JIT compilation is most beneficial for functions that:\n",
    "\n",
    "1. Perform complex numerical computations\n",
    "2. Are called multiple times with inputs of the same shape and dtype\n",
    "3. Don't rely heavily on Python control flow (e.g., if statements, for loops)\n",
    "\n",
    "However, there's a compilation overhead when a JIT-compiled function is first called or when it's called with new input shapes. For very simple or rarely called functions, this overhead might outweigh the benefits.\n",
    "\n",
    "\n",
    "Sometimes, you might want to JIT-compile a function but keep some arguments static (not traced or specialized). You can do this using `static_argnums` or `static_argnames`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd8b6d6",
   "metadata": {},
   "source": [
    "##### JIT and Random Numbers\n",
    "\n",
    "When using random numbers with JIT-compiled functions, you need to be careful about how you handle the random state. JAX uses a functional random number generation system, where you explicitly pass and return PRNG keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db33d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.random as random\n",
    "\n",
    "@jax.jit\n",
    "def f(key, x):\n",
    "    return x + random.normal(key, x.shape)\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "x = jnp.array([1.0, 2.0, 3.0])\n",
    "result = f(key, x)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6bc6bf",
   "metadata": {},
   "source": [
    "#### Common Pitfalls and Best Practices\n",
    "\n",
    "1. **Python Control Flow**: JIT works best with JAX's control flow primitives (`lax.cond`, `lax.while_loop`, etc.) rather than Python control flow.\n",
    "\n",
    "2. **In-place Updates**: Remember that JAX arrays are immutable. In-place updates in JIT-compiled functions won't work as expected.\n",
    "\n",
    "3. **Non-JAX Operations**: JIT-compiled functions should primarily use JAX operations. NumPy operations or other Python functions might not be compatible.\n",
    "\n",
    "4. **Reshaping**: Be cautious about reshaping arrays inside JIT-compiled functions, as this can lead to recompilation.\n",
    "\n",
    "5. **Precision**: JIT compilation might change the order of operations, which can lead to small numerical differences. Use `jax.config.update('jax_enable_x64', True)` if you need higher precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7577748",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "__üìù Exercise__\n",
    "\n",
    "Write a function that computes the mean squared error between two arrays:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    " JIT-compile this function and compare its performance with the non-JIT version.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1f3a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Add your code here ---\n",
    "def mse(y_true, y_pred):\n",
    "    ...\n",
    "  \n",
    "jit_mse = ...\n",
    "# ---------------------------\n",
    "\n",
    "marking.verify_mse(mse)\n",
    "marking.verify_mse(jit_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8274754",
   "metadata": {},
   "source": [
    "### Automatic Differentiation in JAX\n",
    "\n",
    "#### Introduction to Automatic Differentiation\n",
    "\n",
    "Automatic differentiation (AD) is a key feature of JAX, allowing you to compute derivatives of functions efficiently and accurately. This is crucial for many applications in scientific computing, optimization, and machine learning. JAX's AD system can compute gradients, Jacobians, and even higher-order derivatives with ease.\n",
    "\n",
    "The main benefits of automatic differentiation are:\n",
    "\n",
    "1. Efficiency: Computes derivatives faster than numerical differentiation methods\n",
    "2. Accuracy: Provides exact derivatives (up to floating-point precision)\n",
    "3. Flexibility: Works with complex functions and supports higher-order derivatives\n",
    "4. Ease of use: Simple API for computing various types of derivatives\n",
    "\n",
    "#### Computing Gradients with `jax.grad`\n",
    "\n",
    "The most common use of automatic differentiation is computing gradients. In JAX, this is done using the `jax.grad` function. This easier to understand with an example.\n",
    "\n",
    "Assume we have the function\n",
    "\n",
    "$$\n",
    "f(x) = x^2\n",
    "$$\n",
    "\n",
    "The derivative of this function with respect to $x$ is\n",
    "\n",
    "$$\n",
    "\\frac{df}{dx} = 2x\n",
    "$$\n",
    "\n",
    "Let's compute this derivative using `jax.grad`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be5d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def f(x):\n",
    "    return x ** 2\n",
    "\n",
    "df = jax.grad(f)\n",
    "\n",
    "x = 3.0\n",
    "print(f\"f({x}) = {f(x)}\")  # Output: f(3.0) = 9.0\n",
    "print(f\"f'({x}) = {df(x)}\")  # Output: f'(3.0) = 6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9789b0d",
   "metadata": {},
   "source": [
    "As you can see, `grad` returns a function that is exactly $df/dx = 2x$. You can then evaluate this function at any value of $x$ to get the derivative at that point. \n",
    "\n",
    "The important thing to remember is that, like `jit`, the `grad` function __takes a function as input and returns a new function as output__. \n",
    "\n",
    "Let's visualize this with the function\n",
    "\n",
    "$$\n",
    "f(x) = \\sin(x), \\quad \\frac{df}{dx} = \\cos(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f02c871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return jnp.sin(x)\n",
    "  \n",
    "df = jax.grad(f) # Compute the gradient of f\n",
    "df_exact = jnp.cos # Exact derivative of f\n",
    "\n",
    "# Plotting\n",
    "x = jnp.linspace(-2*jnp.pi, 2*jnp.pi, 100)\n",
    "y = f(x)\n",
    "y_exact = df_exact(x)\n",
    "y_autograd = jax.vmap(df)(x)\n",
    "\n",
    "plt.plot(x, y, label=\"f(x) = sin(x)\")\n",
    "plt.plot(x, y_autograd, label=\"f'(x) (autograd)\")\n",
    "plt.plot(x, y_exact, label=\"f'(x) = cos(x)\", linestyle=\"--\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9e5a57",
   "metadata": {},
   "source": [
    "For functions with multiple inputs, `grad` computes the gradient with respect to the first argument by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5dda2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "    return x ** 2 + y ** 3\n",
    "\n",
    "df_dx = jax.grad(f)\n",
    "df_dy = jax.grad(f, argnums=1) # Compute the gradient with respect to the second argument, remember Python is 0-indexed\n",
    "\n",
    "x, y = 2.0, 3.0\n",
    "print(f\"‚àÇf/‚àÇx at (2, 3) = {df_dx(x, y)}\")\n",
    "print(f\"‚àÇf/‚àÇy at (2, 3) = {df_dy(x, y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c7de0d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "__üìù Exercise__\n",
    "\n",
    "1. Write a function that computes the Rosenbrock function\n",
    "\n",
    "$$\n",
    "f(x, y) = (1 - x)^2 + 100(y - x^2)^2\n",
    "$$\n",
    "\n",
    "2. Use `jax.grad` to compute the gradient of this function with respect to x and y\n",
    "3. Evaluate the gradient at the points `(0, 0)`, `(1, 1)`, and `(2, 4)`\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3eef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Add your code here ---\n",
    "def f(p):\n",
    "  \"\"\"p is a 1D array of length 2, representing a point in 2D space.\"\"\"\n",
    "  ...\n",
    "\n",
    "grad_0_0 = ...\n",
    "grad_1_1 = ...\n",
    "grad_2_3 = ...\n",
    "# ---------------------------\n",
    "\n",
    "marking.verify_rosenbrock(jnp.array([0., 0.]), grad_0_0)\n",
    "marking.verify_rosenbrock(jnp.array([1., 1.]), grad_1_1)\n",
    "marking.verify_rosenbrock(jnp.array([2., 3.]), grad_2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23072b45",
   "metadata": {},
   "source": [
    "#### Higher-order gradients\n",
    "\n",
    "JAX also provides functions to compute Jacobians (`jacfwd` and `jacrev`):\n",
    "\n",
    "$$\n",
    "J_{f}(x) = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
    "... & ... & ... & ... \\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1} & \\frac{\\partial f_m}{\\partial x_2} & \\cdots & \\frac{\\partial f_m}{\\partial x_n} \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{m \\times n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fd26fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return jnp.array([x[0] ** 2, x[1] ** 3, x[0] * x[1]])\n",
    "\n",
    "x = jnp.array([2.0, 3.0])\n",
    "\n",
    "# Forward-mode Jacobian\n",
    "J_fwd = jax.jacfwd(f)(x)\n",
    "print(\"Forward-mode Jacobian:\")\n",
    "print(J_fwd)\n",
    "\n",
    "# Reverse-mode Jacobian\n",
    "J_rev = jax.jacrev(f)(x)\n",
    "print(\"Reverse-mode Jacobian:\")\n",
    "print(J_rev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c1c22f",
   "metadata": {},
   "source": [
    "The choice between `jacfwd` and `jacrev` depends on the dimensions of the input and output. Generally, `jacrev` is more efficient when the output dimension is smaller than or equal to the input dimension, and `jacfwd` is more efficient otherwise.\n",
    "\n",
    "You can compute even higher-order derivatives by applying `grad` multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c843ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x ** 4\n",
    "\n",
    "# Third-order derivative\n",
    "d3f_dx3 = jax.grad(jax.grad(jax.grad(f)))\n",
    "\n",
    "x = 2.0\n",
    "print(f\"f'''({x}) = {d3f_dx3(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dfd22e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "__üìù Exercise__\n",
    "\n",
    "1. Write a function that computes the Cartesian coordinates $(x, y, z)$ from spherical coordinates $(r, \\theta, \\phi)$ \n",
    "2. Compute the Jacobian of this function in $(1.0, 0.5, 0.25)$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3ed3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_to_spherical(rŒ∏œÜ):\n",
    "    \"\"\"rŒ∏œÜ is a 1D array of length 3, representing spherical coordinates.\n",
    "    \n",
    "    The function should return a 1D array of length 3, representing Cartesian coordinates.\n",
    "    \"\"\"\n",
    "    # --- Add your code here ---\n",
    "    ...\n",
    "    # --------------------------\n",
    "    return xyz\n",
    "    \n",
    "# Compute Jacobians\n",
    "# --- Add your code here ---\n",
    "J = jax.jacfwd(cartesian_to_spherical)(jnp.array([1.0, 0.5, 0.25]))\n",
    "# --------------------------\n",
    "\n",
    "marking.verify_sphere_to_cart(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e17c48",
   "metadata": {},
   "source": [
    "For a more in-depth look at automatic differentiation in JAX, check out the [Automatic differentiation](https://jax.readthedocs.io/en/latest/automatic-differentiation.html#automatic-differentiation) section of the JAX documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e64b9bf",
   "metadata": {},
   "source": [
    "### Optimization with Automatic Differentiation and `optax`\n",
    "\n",
    "In this section, we'll explore how to use JAX's automatic differentiation capabilities in conjunction with the Optax library to solve a simple optimization problem. We'll use gradient descent to find the minimum of a function, demonstrating the power of combining these tools for efficient optimization.\n",
    "\n",
    "Optax is a gradient processing and optimization library for JAX. It provides a wide range of optimization algorithms and is designed to be flexible and composable. In this example, we'll use a basic gradient descent optimizer, but Optax supports many more advanced algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03de8fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c8d371",
   "metadata": {},
   "source": [
    "Let's consider a simple 2D optimization problem: finding the minimum of the Himmelblau function. This function is often used as a test problem for optimization algorithms because it has multiple local minima.\n",
    "\n",
    "The Himmelblau function is defined as:\n",
    "\n",
    "$$\n",
    "f(x, y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2\n",
    "$$\n",
    "\n",
    "Let's implement this function in JAX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56731ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jwavetutorial import visualize\n",
    "\n",
    "def himmelblau(xy):\n",
    "    x, y = xy\n",
    "    return (x**2 + y - 11)**2 + (x + y**2 - 7)**2\n",
    "\n",
    "# Plot the function\n",
    "visualize.visualize_2d_function(himmelblau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094ed69a",
   "metadata": {},
   "source": [
    "Now, let's use JAX's automatic differentiation and Optax to implement gradient descent for finding the minimum of the Himmelblau function.\n",
    "\n",
    "First of all, let's get the gradient of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82f1b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_himmelblau = jax.grad(himmelblau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d68a29",
   "metadata": {},
   "source": [
    "We will use the `optax` library to implement the gradient descent optimizer. Gradient descent is a simple optimization algorithm that updates the parameters in the direction of the negative gradient:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\alpha \\nabla f(\\theta_t)\n",
    "$$\n",
    "\n",
    "where $\\theta$ is the parameter vector, $\\alpha$ is the learning rate, and $\\nabla f(\\theta)$ is the gradient of the function with respect to the parameters. We will use Adam, which is a more advanced variant of gradient descent that adapts the learning rate for each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a16cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = optax.adam(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f765122",
   "metadata": {},
   "source": [
    "At this point, we need to define the initial parameters and initialize the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e3adb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parameters\n",
    "xy_0 = jnp.array([-0., 0.])  \n",
    "\n",
    "# Initialize the optimizer state\n",
    "opt_state = optimizer.init(xy_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee772ea",
   "metadata": {},
   "source": [
    "Lastly, we need to define what is a single optimization step for gradient descent. This is done by:\n",
    "1. Computing the gradient of the function at the current parameters. Here we will the `value_and_grad` function, a variation of `grad`, to compute both the value and the gradient at the same time.\n",
    "2. Applying the optimizer to update the parameters\n",
    "\n",
    "Note that we are compiling this entire optimization step using `jit` to speed up the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d455f38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a single optimization step\n",
    "@jax.jit\n",
    "def optimization_step(params, opt_state):\n",
    "  # Compute the loss and the gradients\n",
    "  loss, grads = jax.value_and_grad(himmelblau)(params)\n",
    "  \n",
    "  # Update the parameters\n",
    "  updates, opt_state = optimizer.update(grads, opt_state)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  \n",
    "  # Return the updated parameters, optimizer state, and loss\n",
    "  return params, opt_state, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd002777",
   "metadata": {},
   "source": [
    "Now, we just need to apply this optimization step multiple times to find the minimum of the Himmelblau function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e38d09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the optimization\n",
    "num_steps = 500\n",
    "trajectory = [xy_0]  # Store the trajectory of the optimization for visualization\n",
    "losses = []\n",
    "\n",
    "for _ in range(num_steps):\n",
    "  # Perform a single optimization step\n",
    "  xy, opt_state, loss = optimization_step(trajectory[-1], opt_state)\n",
    "  \n",
    "  # Store the updated parameters for later visualization\n",
    "  trajectory.append(xy)\n",
    "  losses.append(loss)\n",
    "  \n",
    "# Display results\n",
    "print(f\"Optimized parameters: {xy}\")\n",
    "print(f\"Minimum value found: {himmelblau(xy)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00272e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.visualize_opt_trajectory(himmelblau, trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1461e7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss over iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e22857",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "__üìù Exercise__\n",
    "\n",
    "Use gradient descent to find the minimum of the Rosenbrock function\n",
    "\n",
    "$$\n",
    "f(x, y) = (1 - x)^2 + 100(y - x^2)^2\n",
    "$$\n",
    "\n",
    "The exercise is solved if the value of the Rosenbrock function at the found minimum is smaller than $10^{-3}$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f0f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Add your code here ---\n",
    "\n",
    "xy_minumum = ...\n",
    "# ---------------------------\n",
    "\n",
    "marking.verify_rosenbrock_optimization(xy_minumum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d0860e-fdbd-4400-a471-1e1aede6fe70",
   "metadata": {},
   "source": [
    "## Acoustic simulations using `jwave`\n",
    "\n",
    "`jwave` is a library of simulators for acoustic applications. It is heavily inspired by [k-Wave](http://www.k-wave.org/)‚Äîa substantial portion of `jwave` is a port of k-Wave in JAX‚Äîand it's intended to be used as a collection of modular blocks that can be easily incorporated into any machine learning pipeline.\n",
    "\n",
    "Embracing the philosophy of [JAX](https://jax.readthedocs.io/en/stable/), j-Wave is developed with the following principles in mind:\n",
    "\n",
    "1. To be differentiable\n",
    "2. To be efficient through `jit` compilation\n",
    "3. To be easily run on GPUs\n",
    "4. To be easily customizable\n",
    "\n",
    "### A simple simulation using `jwave`\n",
    "\n",
    "We will now learn the basic components needed to run a `jwave` simulation.\n",
    "\n",
    "Similarly to k-Wave, j-Wave requires the user to specify a computational domain where the simulation takes place. This is done using the `Domain` [dataclass](https://docs.python.org/3/library/dataclasses.html).\n",
    "\n",
    "The inputs for the constructor are the size of the domain in grid points in each spatial direction, and the corresponding discretization steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad242c-0795-42a5-b563-ac3a9bd773f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jwave.geometry import Domain\n",
    "\n",
    "N, dx = (128, 128), (0.1e-3, 0.1e-3)\n",
    "domain = Domain(N, dx)\n",
    "\n",
    "# Displaying the domain object\n",
    "domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf35f2f7-0e0f-4eca-af7a-5df6de2ca2b6",
   "metadata": {},
   "source": [
    "The second ingredient for a simulation is the acoustic medium, defined using the `Medium` dataclass. In this example, the speed of sound has a background value of $1500m/s$ and a rectangular inclusion of $2000 m/s$, while the density has a constant value of 1000 $kg/m^3$. \n",
    "\n",
    "They are defined as part of the `Medium` dataclass, which also needs the computational domain as mandatory input argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f4fbd4-ad37-4b7c-a34e-38a7a1d6bff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jwave.geometry import Medium\n",
    "\n",
    "# Create the sound speed map\n",
    "c0 = 1500 * jnp.ones(domain.N, dtype=jnp.float32)\n",
    "c0 = c0.at[39:-39, 50:-30].set(2000.)\n",
    "\n",
    "# Define the medium properties\n",
    "medium = Medium(domain=domain, sound_speed=c0, density=1000., attenuation=0.0)\n",
    "print(medium)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd76873-ab64-447e-a2d9-a12c680c45fb",
   "metadata": {},
   "source": [
    "Note that the sound speed is automatically defined as a `FourierSeries` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3445ddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "medium.sound_speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf638fd8-432d-4470-ba16-865a3633fe7b",
   "metadata": {},
   "source": [
    "This is because, by default, `jwave` use Fourier spectral techniques to approximate spatial differential operators. For more information about this, check out the [`jaxdf` documentation](https://ucl-bug.github.io/jaxdf/index.html).\n",
    "\n",
    "For now, all we need to remember is that to access the values of a `FourierSeries` object we can use the `.on_grid` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bbdd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "c0 = medium.sound_speed.on_grid\n",
    "plt.imshow(c0)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f896cd1-3d4d-4ab9-ac47-e1b508608bc9",
   "metadata": {},
   "source": [
    "Lastly, simulations requires to define a `TimeAxis` object, which is used by the timestepping scheme of the numerical simulation. To ensure a stable simulation, this object can be constructed from the medium object for a given [CFL number](https://it.wikipedia.org/wiki/Condizione_di_Courant-Friedrichs-Lewy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04976cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jwave.geometry import TimeAxis\n",
    "\n",
    "time_axis = TimeAxis.from_medium(medium, cfl=0.3)\n",
    "print(time_axis.dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3ec8d7-b3c3-4292-97c9-f94df217ba92",
   "metadata": {},
   "source": [
    "For this initial example, we will do an initial value problem that shows the evolution of an initial pressure distribution over time. \n",
    "\n",
    "Once again, the initial pressure distribution is a `Field`, therefore it must be somehow represented according to a [discretization](https://ucl-bug.github.io/jaxdf/notebooks/api_discretization.html). As metioned, most of the functions of `jwave` are tested using a `FourierSeries` discretization (i.e. spectral methods), so this is the one we will use to define the initial pressure field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2386a567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jwave import FourierSeries\n",
    "from jwave.geometry import circ_mask\n",
    "from jwave.utils import show_field\n",
    "\n",
    "p0_array = 1.0 * jnp.expand_dims(circ_mask(N, 3, (64, 30)), -1)\n",
    "p0 = FourierSeries(p0_array, domain)\n",
    "\n",
    "show_field(p0)\n",
    "plt.title(f\"Initial pressure field\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b613b1-e7b2-4d93-abe9-a8d69d85dfd7",
   "metadata": {},
   "source": [
    "We can finally run the simulation, using the `simulate_wave_propagation` function. Note that this entire function can be `jit` compiled to run on your hardware (including GPUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b0dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit\n",
    "from jwave.acoustics import simulate_wave_propagation\n",
    "\n",
    "@jit\n",
    "def compiled_simulator(medium, p0):\n",
    "    return simulate_wave_propagation(medium, time_axis, p0=p0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ee2703-aca0-4100-9f3c-5dc1da3eb4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pressure = compiled_simulator(medium, p0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8819f576-2e0c-48e4-b95c-7dd104a51083",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 250\n",
    "show_field(pressure[t])\n",
    "plt.title(f\"Pressure field at t={time_axis.to_array()[t]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d351ba5-4a69-406f-b9f8-29390bcd9f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving video\n",
    "from jwave.extras.export import save_video\n",
    "from IPython.display import HTML\n",
    "\n",
    "save_video(pressure, 'sample_video.mp4', vmax=0.05, vmin=-0.05, fps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ae8ef0-cc09-4178-93bb-adac4883135f",
   "metadata": {},
   "source": [
    "Note that we can change the medium and reuse the entire simulation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ba56cf-ece4-4fff-8b71-483736727415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update medium\n",
    "new_medium = Medium(domain=domain, sound_speed=1500., density=1000., attenuation=0.0)\n",
    "\n",
    "# Run simulation\n",
    "new_pressure = compiled_simulator(new_medium, p0)\n",
    "\n",
    "# Show field\n",
    "show_field(new_pressure[t])\n",
    "plt.title(f\"Pressure field at t={time_axis.to_array()[t]}\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24c72aee-c360-4d1d-a335-a3cb9d609f25",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "__üìù Exercise__\n",
    "\n",
    "Write the code to perform the following simulation on a 15x15mm domain\n",
    "\n",
    "![image.png](https://github.com/astanziola/cargese2024/blob/main/images/sim_example.png?raw=true)\n",
    "\n",
    "Use a CFL of 0.2 and display the field at the $t=5ns$. You can make the half-circular inclusion and the source line (and thickness) of arbitrary size. Use 1000 $kg/m^3$ as density map.\n",
    "\n",
    "Explore what happens as you change the various dimensions.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce11ed4e-11b5-4f5c-bbd5-3a29538be583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Add your code here ---\n",
    "...\n",
    "# ---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b07834-f531-49ce-bb23-21e393811c10",
   "metadata": {},
   "source": [
    "### Time Harmonic simulations\n",
    "\n",
    "`jwave` can also solve the time-harmonic acoustic propagation problem, which are governed by the Helmholtz equation.\n",
    "\n",
    "The Helmholtz equation is given by Fourier transforming the wave equation in the temporal domain, which gives\n",
    "\n",
    "$$\n",
    "    \\left(\\nabla +\\frac{\\omega^2}{c^2}\\right)\\phi = i \\omega S_M,\n",
    "$$\n",
    "\n",
    "with $P, S_M \\in C^{2}(\\mathbb{C})$. \n",
    "\n",
    "In `jwave`, the Helmholtz equation solved also takes into account heterogeneous absorption and density, and it is given as\n",
    "$$\n",
    "\\left(\\frac{\\omega^2}{c_0^2} + \\nabla^2  - \\frac{1}{\\rho_0} \\nabla \\rho_0 \\cdot \\nabla  + \\frac{2i\\omega^3\\alpha_0}{c_0} \\right)P = i \\omega S_M\n",
    "$$\n",
    "where:\n",
    "- $\\omega =2\\pi f_0$ is the angular frequency, and $f_0$ is the frequency of the wave\n",
    "- $\\rho_0$ is the material density\n",
    "- $\\alpha_0$ is the attenuation coefficient\n",
    "- $S_M$ is the mass source term\n",
    "\n",
    "Changing between time-varying and time-harmonic simulation can be done easily by just switching solver.\n",
    "\n",
    "Note that the source pressure map needs to be complex in this case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f64902e-84c3-4ee1-ab16-129bf1f5eb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jwave.acoustics.time_harmonic import helmholtz_solver\n",
    "\n",
    "# Defining frequency (2MHz)\n",
    "omega = 2e6 * 2 * jnp.pi\n",
    "\n",
    "# Making source map complex\n",
    "src = p0 + 0j\n",
    "\n",
    "# Solving the time domain problem\n",
    "@jit\n",
    "def solve_helmholtz(medium, src):\n",
    "    return helmholtz_solver(medium, omega, src)\n",
    "\n",
    "field = solve_helmholtz(medium, src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87af8fc8-368c-46d7-af7c-55a83a74453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jwave.utils import display_complex_field\n",
    "\n",
    "_ = display_complex_field(field)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40c67e6-ece7-43f9-b2e2-2eae7d4737e9",
   "metadata": {},
   "source": [
    "### Apply Automatic Differentiation to `jwave` simulations\n",
    "\n",
    "One of the most powerful features of `jwave` is its seamless integration with JAX's automatic differentiation capabilities. This allows us to compute gradients of acoustic simulations with respect to various parameters, opening up a wide range of applications in optimization, inverse problems, and sensitivity analysis.\n",
    "\n",
    "#### Why is Automatic Differentiation Useful in Acoustics?\n",
    "\n",
    "Automatic differentiation in acoustic simulations enables us to:\n",
    "\n",
    "1. Optimize transducer designs for specific acoustic fields\n",
    "2. Solve inverse problems in acoustic imaging\n",
    "3. Perform sensitivity analysis of acoustic fields to medium properties\n",
    "4. Train machine learning models that incorporate acoustic physics\n",
    "\n",
    "Let's explore how to apply automatic differentiation in `jwave` through a series of examples, each building on the last.\n",
    "\n",
    "To warm up, we'll compute the gradient of the pressure field with respect to a global scaling factor of the sound speed.\n",
    "\n",
    "In practice, we are treating the entire simulator as a function that we want to pass to `jax` autodiff capabilities. In this specific case, we are scaling the sound speed by a scaling factor $\\alpha$. You can think about the entire simulator as a mapping from $\\alpha$ to the pressure field:\n",
    "\n",
    "$$\n",
    "p = f(\\alpha), \\qquad f = \\text{simulation function}\n",
    "$$\n",
    "\n",
    "The following code allows to find the gradient (or better, the Jacobian) of such function\n",
    "\n",
    "$$\n",
    "\\frac{\\partial p}{\\partial \\alpha}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0844309-6ef3-49a4-899f-649ba94b6411",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def solve_helmholtz(Œ±, medium, src):\n",
    "    # Scale the sound field\n",
    "    c0 = Œ± * medium.sound_speed\n",
    "\n",
    "    new_medium = Medium(\n",
    "        domain, \n",
    "        sound_speed = c0, \n",
    "        density = medium.density, \n",
    "        attenuation = medium.attenuation\n",
    "    )\n",
    "    \n",
    "    return helmholtz_solver(new_medium, omega, src) \n",
    "\n",
    "dp = jax.jacfwd(solve_helmholtz)\n",
    "\n",
    "grad_field = dp(1.0, medium, src)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e75223-5ac4-4b4b-9a43-d77f73c50657",
   "metadata": {},
   "source": [
    "Let's break this down:\n",
    "\n",
    "1. We define a function `solve_helmholtz` that takes $\\alpha$ as its first argument. This function scales the sound speed of the medium by this factor before solving the Helmholtz equation. This is our $f$.\n",
    "\n",
    "2. We use `jax.jacfwd` to compute the Jacobian of this function with respect to its first argument $\\alpha$. \n",
    "\n",
    "3. We evaluate this gradient at `Œ± = 1.0`, which gives us the sensitivity of the pressure field to small changes in the overall sound speed.\n",
    "\n",
    "The resulting `grad_field` is a complex-valued field that shows how the pressure at each point would change with a small increase in the sound speed. Positive real values indicate that the pressure amplitude would increase, while negative values indicate a decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771b48ff-67a5-41c8-9b15-6ade586a2eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84eff8f-f413-425e-be5c-091d277d0322",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = display_complex_field(grad_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600d90c1-9719-4a25-abec-b026b0bd6ddd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "__üìù Exercise__\n",
    "\n",
    "Assumee that the sound speed is parametrized as $c = \\alpha c_0$. Find the gradient of the pressure field $\\partial p / \\partial \\alpha$ of a time domain simulation and evaluate it at $t=5ns$.\n",
    "\n",
    "For the other parameters, use the same setup as the previous exercise\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203d43a-12a4-4547-b011-6524faca3f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Add your code here ---\n",
    "...\n",
    "# ---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9d3442-e3a8-4d81-9519-b9fd85cf4eb7",
   "metadata": {},
   "source": [
    "### More examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c159554c-d73e-4e9f-90d0-8ce3f38c1cd9",
   "metadata": {},
   "source": [
    "#### Gradient with Respect to Source Position\n",
    "\n",
    "Now, let's consider a more complex example where we compute the gradient of the pressure field with respect to the position of a source. This could be useful for optimizing transducer placement.\n",
    "\n",
    "First, let's define a function to create a localized source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19138c32-9ac2-4ce5-ac02-6869c53c60ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinc_2d(p, domain):\n",
    "    xy = domain.grid\n",
    "    x, y = xy[...,0], xy[...,1]\n",
    "    val = jnp.sinc((p[0] - x)/domain.dx[0]) * jnp.sinc((p[1] - y)/domain.dx[1])\n",
    "\n",
    "    # Make it localized\n",
    "    R = jnp.linalg.norm(p - xy, axis=-1)\n",
    "    val = val * (R < 1e-3)\n",
    "    return FourierSeries(val, domain) + 0j\n",
    "\n",
    "p0 = jnp.asarray([-4e-3, 1.3e-3])\n",
    "src_point = sinc_2d(p0, domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5495ad46-b6e8-4f73-b074-2361b118c35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = display_complex_field(src_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527caff8-55a4-4a5c-b17b-1bec1d27e711",
   "metadata": {},
   "source": [
    "This `sinc_2d` function creates a localized source at a given point `p` in the domain. The sinc function gives a smooth, band-limited representation of the source, and we multiply by a spatial window to ensure it's localized. We do need to have a continuous parametrized function of the variables that we want to optimize for, in order to have meaningful gradients.\n",
    "\n",
    "Now, let's define our simulation function and compute its gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382f2b8e-0953-43b0-8cf8-a844e78805e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def simulate_with_source(source_position, medium):\n",
    "    src = sinc_2d(source_position, domain)\n",
    "    return helmholtz_solver(medium, omega, src)\n",
    "\n",
    "# Compute the gradient of the simulation with respect to source position\n",
    "grad_wrt_position = jax.jacfwd(simulate_with_source)\n",
    "\n",
    "# Evaluate the gradient at our initial source position\n",
    "position_sensitivity = grad_wrt_position(p0, medium)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e73ed8a-2f85-4a4f-b345-3c796944a548",
   "metadata": {},
   "source": [
    "Here's what's happening:\n",
    "\n",
    "1. We define `simulate_with_source`, which takes a `source_position` and creates a localized source at that position before solving the Helmholtz equation.\n",
    "\n",
    "2. We use `jax.jacfwd` to compute the Jacobian of this function with respect to `source_position`. Since `source_position` is a 2D vector (x and y coordinates), the Jacobian will give us the sensitivity of the pressure field to changes in both x and y directions of the source.\n",
    "\n",
    "3. We evaluate this Jacobian at our initial source position `p0`.\n",
    "\n",
    "The resulting `position_sensitivity` is a tensor field. For each point in the domain, it gives a 2x2 matrix representing how the pressure at that point would change with small movements of the source in x and y directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1770f14f-42ba-4323-8827-a060d6892751",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sensitivity = position_sensitivity.on_grid[...,0]\n",
    "y_sensitivity = position_sensitivity.on_grid[...,1]\n",
    "\n",
    "# Visualize sensitivity maps\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,7))\n",
    "ax.flatten()\n",
    "ax[0].imshow(jnp.abs(x_sensitivity), cmap=\"inferno\")\n",
    "ax[0].set_title(\"Abs. sensitivity x axis\")\n",
    "ax[1].imshow(jnp.abs(y_sensitivity), cmap=\"inferno\")\n",
    "ax[1].set_title(\"Abs. sensitivity y axis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6337a3-1a07-4ce6-8355-1660daf46a82",
   "metadata": {},
   "source": [
    "#### Optimizing Source Positions for Maximum Pressure at a Target\n",
    "\n",
    "Building on the previous example, let's use automatic differentiation to optimize the positions of multiple sources to maximize the pressure at a specific target point.\n",
    "\n",
    "First, let's define a function to create a multi-source field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5a04a1-2066-4c3a-81b1-9b1a1d930b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_source(pts, domain):\n",
    "    src_maps = jax.vmap(sinc_2d, in_axes=(0,None))(pts, domain).on_grid\n",
    "    src = jnp.sum(src_maps, axis=0)\n",
    "    return FourierSeries(src, domain)\n",
    "\n",
    "num_sources = 11\n",
    "pts_x = jnp.asarray([-4e-3]*num_sources)\n",
    "pts_y = jnp.linspace(-4e-3, 4e-3, num_sources)\n",
    "pts = jnp.stack([pts_x, pts_y], axis=1)\n",
    "\n",
    "src_map = construct_source(pts, domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c29ea49-11f0-4591-9708-6a85987e0622",
   "metadata": {},
   "source": [
    "This function creates multiple sources at the given positions and sums their contributions to get the total source map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb82c54e-9bad-4b69-bee2-47c78206a86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = display_complex_field(src_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052d4793-8afb-4cc7-9735-245c7fb67bb3",
   "metadata": {},
   "source": [
    "Let's see how the field generated by such sources looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184eabff-1f87-412a-9e26-11cffabba2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_helmholtz(pts, src):\n",
    "    src_map = construct_source(pts, domain)\n",
    "    medium = Medium(domain, 1500., 1000., 0.0)\n",
    "    return helmholtz_solver(medium, omega, src_map) \n",
    "    \n",
    "pts0 = jnp.stack([pts_x, pts_y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998aded3-0351-40ba-abc1-1edbdc6ef449",
   "metadata": {},
   "outputs": [],
   "source": [
    "field = solve_helmholtz(pts0, src)\n",
    "_ = display_complex_field(field)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593ea02e-0978-4435-8cfc-0c415171bb93",
   "metadata": {},
   "source": [
    "Now, let's define our objective function and optimization procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a6feca-5182-4bc4-8387-71d77f72182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_amplitude(pts, src):\n",
    "    field = solve_helmholtz(pts, src)\n",
    "    field_at_target = jnp.abs(field.on_grid[100,80,0])\n",
    "    return field_at_target \n",
    "\n",
    "# Set up optimizer\n",
    "learning_rate = 0.00001\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "# Implement optimization step\n",
    "@jax.jit\n",
    "def optimization_step(params, opt_state, src):\n",
    "    # Compute the loss and the gradients\n",
    "    loss, grads = jax.value_and_grad(target_amplitude)(params, src)\n",
    "    grads = -grads\n",
    "    \n",
    "    # Update the parameters\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    \n",
    "    # Return the updated parameters, optimizer state, and loss\n",
    "    return params, opt_state, loss\n",
    "\n",
    "# Run optimization\n",
    "num_steps = 20\n",
    "pts = jnp.stack([pts_x, pts_y], axis=1)\n",
    "opt_state = optimizer.init(pts)\n",
    "losses = []\n",
    "\n",
    "for n in range(num_steps):\n",
    "    # Perform a single optimization step\n",
    "    pts, opt_state, loss = optimization_step(pts, opt_state, src)\n",
    "\n",
    "    print(f\"Iteration {n}, amplitude: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f48a8ae-533f-4b0b-a5aa-6de5d9b049bc",
   "metadata": {},
   "source": [
    "Let's break down what's happening here:\n",
    "\n",
    "1. We define `target_amplitude`, which computes the amplitude of the pressure field at a specific target point (here, at grid coordinates [100, 80]).\n",
    "\n",
    "2. We set up an Adam optimizer from the `optax` library, which is compatible with JAX's automatic differentiation.\n",
    "\n",
    "3. In `optimization_step`, we use `jax.value_and_grad` to compute both the target amplitude and its gradient with respect to the source positions. We negate the gradients because we want to maximize the amplitude, not minimize it.\n",
    "\n",
    "4. We run the optimization for 20 steps, updating the source positions at each step to increase the pressure amplitude at the target point.\n",
    "\n",
    "This optimization procedure automatically computes the gradients of the entire acoustic simulation with respect to the source positions and uses these gradients to iteratively improve the source configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664c7af2-4a2d-4ae4-b8c9-3fb3d0d96a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_map = construct_source(pts, domain)\n",
    "_ = display_complex_field(src_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66aea68-0b75-416a-b5c1-bc9f008c684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "field = solve_helmholtz(pts, src)\n",
    "_ = display_complex_field(field)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92703577-7885-466d-8a9d-8631d5eaa01a",
   "metadata": {},
   "source": [
    "#### Linear uncertainty propagation\n",
    "\n",
    "For a fixed problem (for example, a particular transducer and density map), the simulated acoustic pressure field $p$ can be written as a function of the parameter vector $\\mathbf \\theta$\n",
    "\n",
    "$$\n",
    "p = f(\\theta)\n",
    "$$\n",
    "\n",
    "Here, $f$ includes the wave simulation, and any other pre or post processing steps needed to generate the output pressure field.\n",
    "\n",
    "Assume now that there's an uncertainty on the parameter $\\theta$, that is\n",
    "\n",
    "$$\n",
    "\\theta =  \\theta_0 + \\varepsilon\n",
    "$$\n",
    "\n",
    "with $\\varepsilon \\sim \\mathcal{N}(0,\\sigma_\\theta^2)$.\n",
    "\n",
    "It might be of interest to ask what is the uncertainty associated with the output of the simulation given the uncertainty of the input parameters. If $\\sigma_\\theta$ is sufficiently small, one way to do so is to expand $f$ as a Taylor series up to second order\n",
    "\n",
    "$$\n",
    "f (\\theta) \\approx f(\\theta_0) + \\frac{\\partial f}{\\partial \\theta}(\\theta - \\theta_0)\n",
    "$$\n",
    "\n",
    "In this case, the variance of the output of the simulation is simply given by \n",
    "\n",
    "$$\n",
    "\\text{Var}(f) = \\sigma_\\theta^2 \\left|\\frac{\\partial f}{\\partial \\theta}\\right|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5268fff6-0434-4b24-8eb8-12cef48bd47a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "__üìù Exercise__\n",
    "\n",
    "Find the approximate standard deviation map for the wavefield amplitude generated by the previous example, if the inclusion sound speed $c_0$ has a mean value of $1800 m/s$ and a standard deviation $\\sigma_c = 50m/s$. Use the optimized transducers positions.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4cdc3b-28a8-44c2-a922-44291e2d6c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Add your code here ---\n",
    "...\n",
    "# ---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a379ae32-04e2-45ed-9f66-2ce52de964a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(std_field, cmap=\"inferno\")\n",
    "plt.title(\"Standard deviation field\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3733b2d6-f96b-470f-90c5-3b5acf650521",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "These examples demonstrate the power and flexibility of automatic differentiation in `jwave`. By leveraging JAX's AD capabilities, we can easily compute gradients of complex acoustic simulations with respect to various parameters. This opens up a wide range of possibilities for acoustic design optimization, inverse problems, and integration with machine learning models.\n",
    "\n",
    "Some potential applications of this technology include:\n",
    "\n",
    "1. Optimizing transducer array designs for focused ultrasound therapy\n",
    "2. Inverse problem solving in ultrasound imaging\n",
    "3. Sensitivity analysis for acoustic cloaking designs\n",
    "4. Training physics-informed neural networks for acoustic problems\n",
    "\n",
    "As you become more comfortable with `jwave` and automatic differentiation, you can explore even more complex scenarios, such as using nested optimization loops, making multi-physics simulations or solving inverse problems in acoustic tomography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e681406b-3cc9-4708-82ac-bb2aa1f994e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
